---
title: "Small model workflow"
output: html_notebook
---

Workflow for small models - the texts are still somewhat incomplete.

"Small" means that the model is fast to fit and we don't have to worry about computation too much.
Once running ~100 fits of the model becomes too costly, there are additional tricks and considerations
that we hope to delve into in a "Complex model workflow" vignette (which currently doesn't exist).

Still many of the approaches here also apply to complex models (especially starting small and building each component separately)

```{r setup, message=FALSE,warning=FALSE, results="hide"}
library(SBC)

# use_cmdstanr <- TRUE # Set to false to use rstan instead
# 
# if(use_cmdstanr) {
#   library(cmdstanr)
# } else {
#   library(rstan)
# }
library(cmdstanr)
library(bayesplot)
library(posterior)

library(future)
plan(multisession) 

options(SBC.min_chunk_size = 5)

```

- Mixture with predictors for ratios

A lot of steps, but I still ignore all the completely invalid models (typos, compile errors, dimension mismatches, ...)

## Mixture component

```{r}
model_first <- cmdstan_model("small_model_workflow/mixture_first.stan")
backend_first <- SBC_backend_cmdstan_sample(model_first) 
```

```{r}
generator_func_first <- function(N) {
  mu1 <- rnorm(1, 3, 1)
  mu2 <- rnorm(1, 3, 1)
  theta <- runif(1)
  
  y <- numeric(N)
  for(n in 1:N) {
    if(runif(1) < theta) {
      y[n] <- rpois(1, exp(mu1))
    } else {
      y[n] <- rpois(1, exp(mu2))
    }
  }
  
  list(
    parameters = list(
      mu1 = mu1,
      mu2 = mu2,
      theta = theta
    ),
    generated = list(
      N = N,
      y = y
    )
  )
}

generator_first <- SBC_generator_function(generator_func_first, N = 50)
```

```{r}
set.seed(68455554)
datasets_first <- generate_datasets(generator_first, 1)
```

```{r}
results_first <- compute_results(datasets_first, backend_first)
```

We have convergence problems, let us examine the pairs plots

```{r}
results_first$stats
mcmc_pairs(results_first$fits[[1]]$draws())
```

One thing that stands out is that either `mu1` is tightly determined and `mu2` is allowed the full prior range or the other way around. We also don't learn anything about theta.

This might be puzzling but relates to bad usage of `log_mix` (TODO explain)

### Fixing mixture

```{r}
model_fixed_log_mix <- cmdstan_model("small_model_workflow/mixture_fixed_log_mix.stan")
backend_fixed_log_mix <- SBC_backend_cmdstan_sample(model_fixed_log_mix)
```

```{r}
results_fixed_log_mix <- compute_results(datasets_first, backend_fixed_log_mix)
```

We see nothing obviously wrong, let's run a few more iterations.

```{r}
results_fixed_log_mix$stats
```
```{r}
set.seed(8314566)
datasets_first_10 <- generate_datasets(generator_first, 10)
```

```{r}
results_fixed_log_mix_2 <- compute_results(datasets_first_10, backend_fixed_log_mix)
```
So there are some problems.

```{r}
hist(results_fixed_log_mix_2$stats$rhat)

```

Let's examine a single pairs plot:

```{r}
mcmc_pairs(results_fixed_log_mix_2$fits[[1]]$draws())
```

We clearly see two modes. And upon reflection, it is not a lot surprising why: swapping `mu1` with `mu2` while also changing `theta` for `1 - theta` gives _exactly_ the same likelihood - because the ordering does not matter. A more detailed explanation of this type of problem is at https://betanalpha.github.io/assets/case_studies/identifying_mixture_models.html

### Fixing ordering

We can easily fix the ordering of the `mu`s by using the `ordered` built-in type.

```{r}
model_fixed_ordered <- cmdstan_model("small_model_workflow/mixture_fixed_ordered.stan")
backend_fixed_ordered <- SBC_backend_cmdstan_sample(model_fixed_ordered) 
```
We also need to update the generator to match the new names and ordering constant:

```{r}
generator_func_ordered <- function(N) {
  # If the priors for all components of an ordered vector are the same
  # then just sorting the result of a generator is enough to create
  # a valid sample from the ordered vector
  mu <- sort(rnorm(2, 3, 1)) 
  theta <- runif(1)
  
  y <- numeric(N)
  for(n in 1:N) {
    if(runif(1) < theta) {
      y[n] <- rpois(1, exp(mu[1]))
    } else {
      y[n] <- rpois(1, exp(mu[2]))
    }
  }
  
  list(
    parameters = list(
      mu = mu,
      theta = theta
    ),
    generated = list(
      N = N,
      y = y
    )
  )
}

generator_ordered <- SBC_generator_function(generator_func_ordered, N = 50)
```

We are kind of confident (and the model fits quickly), so we'll already start with 10 datasets.

```{r}
set.seed(3785432)
datasets_ordered_10 <- generate_datasets(generator_ordered, 10)
```


```{r}
results_fixed_ordered <- compute_results(datasets_ordered_10, backend_fixed_ordered)
```


```{r}
results_fixed_ordered$stats
```

There are fits with high R-hats and low ESS. Let's look at the pairs plot:

```{r}
problematic_fit_id <- 2
problematic_fit <- results_fixed_ordered$fits[[problematic_fit_id]]
mcmc_pairs(problematic_fit$draws(), np = nuts_params(problematic_fit))
```

There is a lot of ugly stuff going on. Notably, one can notice that the posterior of theta is bimodal, preferring either almost 0 or almost 1 - and when that happens, the mean of one of the components is almost unconstrained. 
Why does that happen? The key to the answer is in the simulated values for the component means:

```{r}
subset_draws(datasets_ordered_10$parameters, draw = problematic_fit_id)
```

We were unlucky enough to simulate a dataset where both components have almost the same mean and thus we are actually looking at a dataset that is not really a mixture. Mixture models can misbehave badly in such cases (see once again the https://betanalpha.github.io/assets/case_studies/identifying_mixture_models.html#5_singular_components_and_computational_issues) for a bit more detailed dive into this particular problem.

### Fixing degenerate components?

What to do about this? Fixing the model to handle such cases gracefully is hard. But the problem is basically our prior - we want to express that (since we are fitting a two component model), we don't expect the means to be too similar. So if we can change our simulation to avoid this, we'll be able to proceed with SBC. If such a pattern appeared in real data, we would still have a problem, but we would notice thanks to the diagnostics.

This can definitely be done. But another way is to just ignore the datasets that had divergences for SBC calculations. It turns out that if we remove datasets in a way that only depends on the observed data (and not on unobserved parameters), the SBC identity is preserved and we can use SBC without modifications. The resulting check is however telling us something only for datasets that were not rejected. In this case this is not a big issue: if a fit had divergent transitions, we would not trust it anyway, so removing fits with divergent transitions is not such a big deal.

For more details see the `rejection_sampling` vignette.

So let us subset the results:

```{r}
dataset_ids_to_keep <- results_fixed_ordered$backend_diagnostics$dataset_id[results_fixed_ordered$backend_diagnostics$n_divergent == 0]

# Equivalent tidy version if you prefer
# dataset_ids_to_keep <- results_fixed_ordered$backend_diagnostics %>% 
#   dplyr::filter(n_divergent == 0) %>%
#   dplyr::pull(dataset_id)


results_fixed_ordered_subset <- results_fixed_ordered[dataset_ids_to_keep]
summary(results_fixed_ordered_subset)
```

This gives us no obvious problems.

```{r}
plot_ecdf_diff(results_fixed_ordered_subset)
```

```{r}
plot_coverage(results_fixed_ordered_subset)

```


So we can run for more iterations:

```{r}
datasets_ordered_100 <- generate_datasets(generator_ordered, 100)
results_fixed_ordered_100 <- compute_results(datasets_ordered_100, backend_fixed_ordered)
```

Once again we subset to keep only non-divergent fits

```{r}
dataset_ids_to_keep <- results_fixed_ordered_100$backend_diagnostics$dataset_id[results_fixed_ordered_100$backend_diagnostics$n_divergent == 0]

# Equivalent tidy version
# dataset_ids_to_keep <- results_fixed_ordered_100$backend_diagnostics %>% 
#   dplyr::filter(n_divergent == 0) %>%
#   dplyr::pull(dataset_id)


results_fixed_ordered_100_subset <- results_fixed_ordered_100[dataset_ids_to_keep]
summary(results_fixed_ordered_100_subset)
```


And combine with the previous fits to not waste our computational effort.

```{r}
results_fixed_ordered_combined <- bind_results(results_fixed_ordered_subset, results_fixed_ordered_100_subset)
plot_ecdf_diff(results_fixed_ordered_combined)
```

Seems fairly well within the expected bounds. We could definitely run more iterations if we wanted to have a more strict check, but for now, we are happy.

Note: it turns out that extending the model to more components becomes somewhat tricky as the model can become sensitive to initialization and the problem of data that can be explained by fewer components than the model has becomes more prevalent.

## Beta regression component

Maybe treating this as a logistic regression component would have been wiser. But since I actually realized that only after spending some time with the beta regression task, I am gonna keep it in - it just demonstrates that a real workflow can be messy.

```{r}
model_beta_first <- cmdstan_model("small_model_workflow/beta_first.stan")
backend_beta_first <- SBC_backend_cmdstan_sample(model_beta_first) 

```

```{r}
generator_func_beta_first <- function(N_obs, N_predictors) {
  repeat {
    beta <- matrix(rnorm(N_predictors * 2, 0, 1), nrow = 2, ncol = N_predictors)
  
    x <- matrix(rnorm(N_predictors * N_obs, 0, 1), nrow = N_predictors, ncol = N_obs)
    x[1, ] <- 1 # Intercept
  
    y <- array(NA_real_, N_obs)
      
    for(n in 1:N_obs) {
      linpred <- rep(0, 2)
      for(c in 1:2) {
        for(p in 1:N_predictors) {
          linpred[c] <- linpred[c] + x[p, n] * beta[c, p]
        }
      }
      y[n] <- rbeta(1, exp(linpred[1]), exp(linpred[2]))
    }
    if(all(y > 1e-7) && all(y < 1 - 1e-7)) {
      break;
    }
  }
    
  list(
    parameters = list(
      beta = beta
    ),
    generated = list(
      N_obs = N_obs,
      N_predictors = N_predictors,
      y = y,
      x = x
    )
  )
}

generator_beta_first <- SBC_generator_function(generator_func_beta_first, N_obs = 50, N_predictors = 3)
```


```{r}
set.seed(3325488)
datasets_beta_first <- generate_datasets(generator_beta_first, 10)
```



```{r}
results_beta_first_10 <- compute_results(datasets_beta_first, backend_beta_first)
```
```{r}
plot_ecdf_diff(results_beta_first_10)
```


```{r}
mcmc_pairs(results_beta_first_10$fits[[3]]$draws())
```

This is a very ugly plot, but we see some correlations between the corresponding beta, let's have a closer look.

```{r}
for(i in 1:5) {
  fit <- results_beta_first_10$fits[[i]]
  print(mcmc_pairs(fit$draws(), pars = c("beta[1,1]", "beta[2,1]","beta[1,2]", "beta[2,2]"), np = nuts_params(fit)))
}
```

Turns out the correlations are in all fits, althoug sometimes they are relatively weak and the sampler is able to handle the posterior, it is potentially troubling. The main issue is that we plan to integrate this model with other components and problems that can be tolerated in a single component might interact with other components and become problematic.


We can even understand the reason for the positive correlation - it is because mean of beta distribution is $\frac{\alpha}{\alpha + \beta}$...

We can also decide whether to keep the full flexibility and allow predictors for precision.

### Parametrizing the beta distribution via mean

This also makes much more sense for the bigger task - combining with the mixture component.

```{r}
model_beta_precision <- cmdstan_model("small_model_workflow/beta_precision.stan")
backend_beta_precision <- SBC_backend_cmdstan_sample(model_beta_precision) 

```

```{r}
generator_func_beta_precision <- function(N_obs, N_predictors) {
  repeat {
    beta <- rnorm(N_predictors, 0, 1)
    phi <- rlnorm(1, 3, 1)
  
    x <- matrix(rnorm(N_predictors * N_obs, 0, 1), nrow = N_predictors, ncol = N_obs)
    x[1, ] <- 1 # Intercept
  
    y <- array(NA_real_, N_obs)
      
    for(n in 1:N_obs) {
      linpred <- 0
      for(p in 1:N_predictors) {
        linpred <- linpred + x[p, n] * beta[p]
      }
      mu <- plogis(linpred)
      y[n] <- rbeta(1, mu * phi, (1 - mu) * phi)
    }
    if(all(y > 1e-7) && all(y < 1 - 1e-7)) {
      break;
    }
  }
    
  list(
    parameters = list(
      beta = beta,
      phi = phi
    ),
    generated = list(
      N_obs = N_obs,
      N_predictors = N_predictors,
      y = y,
      x = x
    )
  )
}

generator_beta_precision <- SBC_generator_function(generator_func_beta_precision, N_obs = 50, N_predictors = 3)
```


```{r}
set.seed(46988234)
datasets_beta_precision_10 <- generate_datasets(generator_beta_precision, 10)
```



```{r}
results_beta_precision_10 <- compute_results(datasets_beta_precision_10, backend_beta_precision)
```

```{r}
plot_ecdf_diff(results_beta_precision_10)
```

```{r}
datasets_beta_precision_90 <- generate_datasets(generator_beta_precision, 90)
results_beta_precision_100 <-
  bind_results(
    results_beta_precision_10,
    compute_results(datasets_beta_precision_90, backend_beta_precision))

datasets_beta_precision_100 <- bind_datasets(datasets_beta_precision_10, datasets_beta_precision_90)
plot_ecdf_diff(results_beta_precision_100)
```


Missing prior!

This type of problem is often not very well visible from SBC (see the `limits_of_SBC` vignette for more detailed discussion)

### Adding missing prior

```{r}
model_beta_precision_fixed_prior <- cmdstan_model("small_model_workflow/beta_precision_fixed_prior.stan")
backend_beta_precision_fixed_prior <- SBC_backend_cmdstan_sample(model_beta_precision_fixed_prior) 

```
```{r}
results_beta_precision_fixed_prior <- compute_results(datasets_beta_precision_100, backend_beta_precision_fixed_prior)

plot_ecdf_diff(results_beta_precision_fixed_prior)
```

```{r}
datasets_beta_precision_100b <- generate_datasets(generator_beta_precision, 100)
results_beta_precision_fixed_prior_200 <-
  bind_results(
    results_beta_precision_fixed_prior,
    compute_results(datasets_beta_precision_100b, backend_beta_precision_fixed_prior))

plot_ecdf_diff(results_beta_precision_fixed_prior_200)

```


## Putting it together


```{r}
model_combined <- cmdstan_model("small_model_workflow/combined_first.stan")
backend_combined <- SBC_backend_cmdstan_sample(model_combined)
```


```{r}
generator_func_combined <- function(N_obs, N_predictors) {
  # If the priors for all components of an ordered vector are the same
  # then just sorting the result of a generator is enough to create
  # a valid sample from the ordered vector
  mu <- sort(rnorm(2, 3, 1)) 
  
  beta <- rnorm(N_predictors, 0, 1)

  x <- matrix(rnorm(N_predictors * N_obs, 0, 1), nrow = N_predictors, ncol = N_obs)
  x[1, ] <- 1 # Intercept

  y <- array(NA_real_, N_obs)

  for(n in 1:N_obs) {
    linpred <- 0
    for(p in 1:N_predictors) {
      linpred <- linpred + x[p, n] * beta[p]
    }
    theta <- plogis(linpred)
    
    if(runif(1) < theta) {
      y[n] <- rpois(1, exp(mu[1]))
    } else {
      y[n] <- rpois(1, exp(mu[2]))
    }
    
  }


  list(
    parameters = list(
      beta = beta,
      mu = mu
    ),
    generated = list(
      N_obs = N_obs,
      N_predictors = N_predictors,
      y = y,
      x = x
    )
  )
}

generator_combined <- SBC_generator_function(generator_func_combined, N_obs = 50, N_predictors = 3)
```

```{r}
set.seed(5749955)
dataset_combined <- generate_datasets(generator_combined, 200)
```

```{r}
results_combined <- compute_results(dataset_combined, backend_combined)
```

```{r}
plot_ecdf_diff(results_combined)
```

### Adding rejection sampling

```{r}
fanos <- vapply(dataset_combined$generated, function(dataset) { var(dataset$y) / mean(dataset$y) }, FUN.VALUE = 0)
plot(fanos, results_combined$backend_diagnostics$n_divergent)
hist(fanos[results_combined$backend_diagnostics$n_divergent > 0])
```


```{r}
generator_func_combined_reject <- function(N_obs, N_predictors) {
  if(N_obs < 5) {
    stop("Too low N_obs for this simulator")
  }
  repeat {
    # If the priors for all components of an ordered vector are the same
    # then just sorting the result of a generator is enough to create
    # a valid sample from the ordered vector
    mu <- sort(rnorm(2, 3, 1)) 
    
    beta <- rnorm(N_predictors, 0, 1)

    x <- matrix(rnorm(N_predictors * N_obs, 0, 1), nrow = N_predictors, ncol = N_obs)
    x[1, ] <- 1 # Intercept
  
    y <- array(NA_real_, N_obs)

    for(n in 1:N_obs) {
      linpred <- 0
      for(p in 1:N_predictors) {
        linpred <- linpred + x[p, n] * beta[p]
      }
      theta <- plogis(linpred)
      
      if(runif(1) < theta) {
        y[n] <- rpois(1, exp(mu[1]))
      } else {
        y[n] <- rpois(1, exp(mu[2]))
      }
      
    }
    if(var(y) / mean(y) > 1.5) {
      break;
    }
  }
    
  list(
    parameters = list(
      beta = beta,
      mu = mu
    ),
    generated = list(
      N_obs = N_obs,
      N_predictors = N_predictors,
      y = y,
      x = x
    )
  )
}

generator_combined_reject <- SBC_generator_function(generator_func_combined_reject, N_obs = 50, N_predictors = 3)
```

```{r}
set.seed(44685226)
dataset_combined_reject <- generate_datasets(generator_combined_reject, 200)
```



```{r}
results_combined_reject <- compute_results(dataset_combined_reject, backend_combined)
```

```{r}
plot_ecdf_diff(results_combined_reject)
```


```{r}
set.seed(1395367854)
dataset_combined_reject_more <- generate_datasets(generator_combined_reject, 300) 
results_combined_reject_more <- bind_results(
  results_combined_reject,
  compute_results(dataset_combined_reject_more, backend_combined)
)

plot_ecdf_diff(results_combined_reject_more)
```

