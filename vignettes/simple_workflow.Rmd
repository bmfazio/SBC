---
title: "Simple model workflow"
output: html_notebook
---

```{r setup}
library(SBC)

# use_cmdstanr <- TRUE # Set to false to use rstan instead
# 
# if(use_cmdstanr) {
#   library(cmdstanr)
# } else {
#   library(rstan)
# }
library(cmdstanr)
library(bayesplot)
library(posterior)

library(future)
plan(multisession) 

options(SBC.min_chunk_size = 5)

```

- Mixture with predictors for ratios

## Mixture component

```{r}
model_first <- cmdstan_model("simple_workflow/mixture_first.stan")
backend_first <- cmdstan_sample_SBC_backend(model_first) 
```

```{r}
generator_func_first <- function(N) {
  mu1 <- rnorm(1, 3, 1)
  mu2 <- rnorm(1, 3, 1)
  theta <- runif(1)
  
  y <- numeric(N)
  for(n in 1:N) {
    if(runif(1) < theta) {
      y[n] <- rpois(1, exp(mu1))
    } else {
      y[n] <- rpois(1, exp(mu2))
    }
  }
  
  list(
    parameters = list(
      mu1 = mu1,
      mu2 = mu2,
      theta = theta
    ),
    generated = list(
      N = N,
      y = y
    )
  )
}

generator_first <- function_SBC_generator(generator_func_first, N = 50)
```

```{r}
set.seed(68455554)
datasets_first <- generate_datasets(generator_first, 1)
```

```{r}
results_first <- compute_results(datasets_first, backend_first)
```

We have convergence problems, let us examine the pairs plots

```{r}
results_first$stats
mcmc_pairs(results_first$fits[[1]]$draws())
```

One thing that stands out is that either `mu1` is tightly determined and `mu2` is allowed the full prior range or the other way around. We also don't learn anything about theta.

This might be puzzling but relates to bad usage of `log_mix` (TODO explain)

### Fixing mixture

```{r}
model_fixed_log_mix <- cmdstan_model("simple_workflow/mixture_fixed_log_mix.stan")
backend_fixed_log_mix <- cmdstan_sample_SBC_backend(model_fixed_log_mix)
```

```{r}
results_fixed_log_mix <- compute_results(datasets_first, backend_fixed_log_mix)
```

We see nothing obviously wrong, let's run a few more iterations.

```{r}
results_fixed_log_mix$stats
```
```{r}
set.seed(8314566)
datasets_first_10 <- generate_datasets(generator_first, 10)
```

```{r}
results_fixed_log_mix_2 <- compute_results(datasets_first_10, backend_fixed_log_mix)
```
So there are some problems.

```{r}
hist(results_fixed_log_mix_2$stats$rhat)

```

Let's examine a single pairs plot:

```{r}
mcmc_pairs(results_fixed_log_mix_2$fits[[1]]$draws())
```

We clearly see two modes. And upon reflection, it is not a lot surprising why: swapping `mu1` with `mu2` while also changing `theta` for `1 - theta` gives _exactly_ the same likelihood - because the ordering does not matter. A more detailed explanation of this type of problem is at https://betanalpha.github.io/assets/case_studies/identifying_mixture_models.html

### Fixing ordering

We can easily fix the ordering of the `mu`s by using the `ordered` built-in type.

```{r}
model_fixed_ordered <- cmdstan_model("simple_workflow/mixture_fixed_ordered.stan")
backend_fixed_ordered <- cmdstan_sample_SBC_backend(model_fixed_ordered) 
```
We also need to update the generator to match the new names and ordering constant:

```{r}
generator_func_ordered <- function(N) {
  # If the priors for all components of an ordered vector are the same
  # then just sorting the result of a generator is enough to create
  # a valid sample from the ordered vector
  mu <- sort(rnorm(2, 3, 1)) 
  theta <- runif(1)
  
  y <- numeric(N)
  for(n in 1:N) {
    if(runif(1) < theta) {
      y[n] <- rpois(1, exp(mu[1]))
    } else {
      y[n] <- rpois(1, exp(mu[2]))
    }
  }
  
  list(
    parameters = list(
      mu = mu,
      theta = theta
    ),
    generated = list(
      N = N,
      y = y
    )
  )
}

generator_ordered <- function_SBC_generator(generator_func_ordered, N = 50)
```

We are kind of confident (and the model fits quickly), so we'll already start with 10 datasets.

```{r}
set.seed(3785432)
datasets_ordered_10 <- generate_datasets(generator_ordered, 10)
```


```{r}
results_fixed_ordered <- compute_results(datasets_ordered_10, backend_fixed_ordered)
```


```{r}
results_fixed_ordered$stats
```
There are fits with high R-hats and low ESS. Let's look at the pairs plot:

```{r}
problematic_fit_id <- 2
problematic_fit <- results_fixed_ordered$fits[[problematic_fit_id]]
mcmc_pairs(problematic_fit$draws(), np = nuts_params(problematic_fit))
```

There is a lot of ugly stuff going on. Notably, one can notice that the posterior of theta is bimodal, preferring either almost 0 or almost 1 - and when that happens, the mean of one of the components is almost unconstrained. 
Why does that happen? The key to the answer is in the simulated values for the component means:

```{r}
subset_draws(datasets_ordered_10$parameters, draw = problematic_fit_id)
```
We were unlucky enough to simulate a dataset where both components have almost the same mean and thus we are actually looking at a dataset that is not really a mixture. Mixture models can misbehave badly in such cases (see once again the https://betanalpha.github.io/assets/case_studies/identifying_mixture_models.html#5_singular_components_and_computational_issues) for a bit more detailed dive into this particular problem.

### Fixing degenerate components?

What to do about this? Fixing the model to handle such cases gracefully is hard. But the problem is basically our prior - we want to express that (since we are fitting a two component model), we don't expect the means to be too similar. So if we can change our simulation to avoid this, we'll be able to proceed with SBC. If such a pattern appeared in real data, we would still have a problem, but we would notice thanks to the diagnostics.

This can definitely be done. But another way is to just ignore the datasets that had divergences for SBC calculations. It turns out that if we remove datasets in a way that only depends on the observed data (and not on unobserved parameters), the SBC identity is preserved and we can use SBC without modifications. The resulting check is however telling us something only for datasets that were not rejected. In this case this is not a big issue: if a fit had divergent transitions, we would not trust it anyway, so removing fits with divergent transitions is not such a big deal.

For more details see the `rejection_sampling` vignette.

So let us subset the results:

```{r}
# Tidy version
run_ids_to_keep <- results_fixed_ordered$backend_diagnostics %>% 
  dplyr::filter(n_divergent == 0) %>%
  dplyr::pull(run_id)
# Equivalent base R code
run_ids_to_keep <- results_fixed_ordered$backend_diagnostics$run_id[results_fixed_ordered$backend_diagnostics$n_divergent == 0]

results_fixed_ordered_subset <- results_fixed_ordered[run_ids_to_keep]
summary(results_fixed_ordered_subset)
```

This gives us no obvious problems.

```{r}
plot_ecdf_diff(results_fixed_ordered_subset)
```
So we can run for more iterations:

```{r}
datasets_ordered_100 <- generate_datasets(generator_ordered, 100)
results_fixed_ordered_100 <- compute_results(datasets_ordered_100, backend_fixed_ordered)
```

Once again we subset to keep only non-divergent fits

```{r}
run_ids_to_keep <- results_fixed_ordered_100$backend_diagnostics %>% 
  dplyr::filter(n_divergent == 0) %>%
  dplyr::pull(run_id)


results_fixed_ordered_100_subset <- results_fixed_ordered_100[run_ids_to_keep]
summary(results_fixed_ordered_100_subset)
```


And combine with the previous fits to not waste our computational effort.

```{r}
results_fixed_ordered_combined <- bind_results(results_fixed_ordered_subset, results_fixed_ordered_100_subset)
plot_ecdf_diff(results_fixed_ordered_combined)
```
Seems fairly well within the expected bounds. We could definitely run more iterations if we wanted to have a more strict check, but for now, we are happy.

Note: it turns out that extending the model to more components becomes somewhat tricky as the model can become sensitive to initialization and the problem of data that can be explained by fewer components than the model has becomes more prevalent.

## Beta regression component

```{r}
model_beta_first <- cmdstan_model("simple_workflow/beta_first.stan")
backend_beta_first <- cmdstan_sample_SBC_backend(model_beta_first) 

```

```{r}
generator_func_beta_first <- function(N_obs, N_predictors) {
  repeat {
    beta <- matrix(rnorm(N_predictors * 2, 0, 1), nrow = 2, ncol = N_predictors)
  
    x <- matrix(rnorm(N_predictors * N_obs, 0, 1), nrow = N_predictors, ncol = N_obs)
    x[1, ] <- 1 # Intercept
  
    y <- array(NA_real_, N_obs)
      
    for(n in 1:N_obs) {
      linpred <- rep(0, 2)
      for(c in 1:2) {
        for(p in 1:N_predictors) {
          linpred[c] <- linpred[c] + x[p, n] * beta[c, p]
        }
      }
      y[n] <- rbeta(1, exp(linpred[1]), exp(linpred[2]))
    }
    if(all(y > 0) && all(y < 1)) {
      break;
    }
  }
    
  list(
    parameters = list(
      beta = beta
    ),
    generated = list(
      N_obs = N_obs,
      N_predictors = N_predictors,
      y = y,
      x = x
    )
  )
}

generator_beta_first <- function_SBC_generator(generator_func_beta_first, N_obs = 50, N_predictors = 3)
```


```{r}
set.seed(3325488)
datasets_beta_first <- generate_datasets(generator_beta_first, 10)
```


```{r}
hist(datasets_beta_first$generated[[1]]$y)
hist(datasets_beta_first$generated[[2]]$y)
hist(datasets_beta_first$generated[[3]]$y)
```


```{r}
results_beta_first_1 <- compute_results(datasets_beta_first[1], backend_beta_first)
```


- Non-identifiability: predictors for all
- Off-by-one: not informed parameter (posterior shrinkage)
- Forgotten prior for intercept?

## Putting it together

TODO: idexing error
