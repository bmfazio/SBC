---
title: "Simple model workflow"
output: html_notebook
---

```{r setup}
library(SBC)

# use_cmdstanr <- TRUE # Set to false to use rstan instead
# 
# if(use_cmdstanr) {
#   library(cmdstanr)
# } else {
#   library(rstan)
# }
library(cmdstanr)
library(bayesplot)
library(posterior)

library(future)
plan(multisession) 

options(SBC.min_chunk_size = 5)

```

- Mixture with predictors for ratios

## Mixture component

```{r}
model_first <- cmdstan_model("simple_workflow/mixture_first.stan")
backend_first <- cmdstan_sample_SBC_backend(model_first) 
```

```{r}
generator_func_first <- function(N) {
  mu1 <- rnorm(1, 3, 1)
  mu2 <- rnorm(1, 3, 1)
  theta <- runif(1)
  
  y <- numeric(N)
  for(n in 1:N) {
    if(runif(1) < theta) {
      y[n] <- rpois(1, exp(mu1))
    } else {
      y[n] <- rpois(1, exp(mu2))
    }
  }
  
  list(
    parameters = list(
      mu1 = mu1,
      mu2 = mu2,
      theta = theta
    ),
    generated = list(
      N = N,
      y = y
    )
  )
}

generator_first <- function_SBC_generator(generator_func_first, N = 50)
```

```{r}
set.seed(68455554)
datasets_first <- generate_datasets(generator_first, 1)
```

```{r}
results_first <- compute_results(datasets_first, backend_first)
```

We have convergence problems, let us examine the pairs plots

```{r}
results_first$stats
mcmc_pairs(results_first$fits[[1]]$draws())
```

One thing that stands out is that either `mu1` is tightly determined and `mu2` is allowed the full prior range or the other way around. We also don't learn anything about theta.

This might be puzzling but relates to bad usage of `log_mix` (TODO explain)

### Fixing mixture

```{r}
model_fixed_log_mix <- cmdstan_model("simple_workflow/mixture_fixed_log_mix.stan")
backend_fixed_log_mix <- cmdstan_sample_SBC_backend(model_fixed_log_mix)
```

```{r}
results_fixed_log_mix <- compute_results(datasets_first, backend_fixed_log_mix)
```

We see nothing obviously wrong, let's run a few more iterations.

```{r}
results_fixed_log_mix$stats
```
```{r}
set.seed(8314566)
datasets_first_10 <- generate_datasets(generator_first, 10)
```

```{r}
results_fixed_log_mix_2 <- compute_results(datasets_first_10, backend_fixed_log_mix)
```
So there are some problems.

```{r}
hist(results_fixed_log_mix_2$stats$rhat)

```

Let's examine a single pairs plot:

```{r}
mcmc_pairs(results_fixed_log_mix_2$fits[[1]]$draws())
```

We clearly see two modes. And upon reflection, it is not a lot surprising why: swapping `mu1` with `mu2` while also changing `theta` for `1 - theta` gives _exactly_ the same likelihood - because the ordering does not matter. A more detailed explanation of this type of problem is at https://betanalpha.github.io/assets/case_studies/identifying_mixture_models.html

### Fixing ordering

We can easily fix the ordering of the `mu`s by using the `ordered` built-in type.

```{r}
model_fixed_ordered <- cmdstan_model("simple_workflow/mixture_fixed_ordered.stan")
backend_fixed_ordered <- cmdstan_sample_SBC_backend(model_fixed_ordered) 
```
We also need to update the generator to match the new names and ordering constant:

```{r}
generator_func_ordered <- function(N) {
  # If the priors for all components of an ordered vector are the same
  # then just sorting the result of a generator is enough to create
  # a valid sample from the ordered vector
  mu <- sort(rnorm(2, 3, 1)) 
  theta <- runif(1)
  
  y <- numeric(N)
  for(n in 1:N) {
    if(runif(1) < theta) {
      y[n] <- rpois(1, exp(mu[1]))
    } else {
      y[n] <- rpois(1, exp(mu[2]))
    }
  }
  
  list(
    parameters = list(
      mu = mu,
      theta = theta
    ),
    generated = list(
      N = N,
      y = y
    )
  )
}

generator_ordered <- function_SBC_generator(generator_func_ordered, N = 50)
```

We are kind of confident (and the model fits quickly), so we'll already start with 10 datasets.

```{r}
set.seed(3785432)
datasets_ordered_10 <- generate_datasets(generator_ordered, 10)
```


```{r}
results_fixed_ordered <- compute_results(datasets_ordered_10, backend_fixed_ordered)
```


```{r}
results_fixed_ordered$stats
```
There are fits with high R-hats and low ESS. Let's look at the pairs plot:

```{r}
problematic_fit_id <- 2
problematic_fit <- results_fixed_ordered$fits[[problematic_fit_id]]
mcmc_pairs(problematic_fit$draws(), np = nuts_params(problematic_fit))
```

There is a lot of ugly stuff going on. Notably, one can notice that the posterior of theta is bimodal, preferring either almost 0 or almost 1 - and when that happens, the mean of one of the components is almost unconstrained. 
Why does that happen? The key to the answer is in the simulated values for the component means:

```{r}
subset_draws(datasets_ordered_10$parameters, draw = problematic_fit_id)
```
We were unlucky enough to simulate a dataset where both components have almost the same mean and thus we are actually looking at a dataset that is not really a mixture. Mixture models can misbehave badly in such cases (see once again the https://betanalpha.github.io/assets/case_studies/identifying_mixture_models.html#5_singular_components_and_computational_issues) for a bit more detailed dive into this particular problem.

### Fixing degenerate components?

What to do about this? Fixing the model to handle such cases gracefully is hard. But the problem is basically our prior - we want to express that (since we are fitting a two component model), we don't expect the means to be too similar. So if we can change our simulation to avoid this, we'll be able to proceed with SBC. If such a pattern appeared in real data, we would still have a problem, but we would notice thanks to the diagnostics.

This can definitely be done. But another way is to just ignore the datasets that had divergences for SBC calculations. It turns out that if we remove datasets in a way that only depends on the observed data (and not on unobserved parameters), the SBC identity is preserved and we can use SBC without modifications. The resulting check is however telling us something only for datasets that were not rejected. In this case this is not a big issue: if a fit had divergent transitions, we would not trust it anyway, so removing fits with divergent transitions is not such a big deal.

For more details see the `rejection_sampling` vignette.

So let us subset the results:

```{r}
# Tidy version
run_ids_to_keep <- results_fixed_ordered$backend_diagnostics %>% 
  dplyr::filter(n_divergent == 0) %>%
  dplyr::pull(run_id)
# Equivalent base R code
run_ids_to_keep <- results_fixed_ordered$backend_diagnostics$run_id[results_fixed_ordered$backend_diagnostics$n_divergent == 0]

results_fixed_ordered_subset <- results_fixed_ordered[run_ids_to_keep]
summary(results_fixed_ordered_subset)
```

This gives us no obvious problems.

```{r}
plot_ecdf_diff(results_fixed_ordered_subset)
```
So we can run for more iterations:

```{r}
datasets_ordered_100 <- generate_datasets(generator_ordered, 100)
results_fixed_ordered_100 <- compute_results(datasets_ordered_100, backend_fixed_ordered)
```

Once again we subset to keep only non-divergent fits

```{r}
run_ids_to_keep <- results_fixed_ordered_100$backend_diagnostics %>% 
  dplyr::filter(n_divergent == 0) %>%
  dplyr::pull(run_id)


results_fixed_ordered_100_subset <- results_fixed_ordered_100[run_ids_to_keep]
summary(results_fixed_ordered_100_subset)
```


And combine with the previous fits to not waste our computational effort.

```{r}
results_fixed_ordered_combined <- bind_results(results_fixed_ordered_subset, results_fixed_ordered_100_subset)
plot_ecdf_diff(results_fixed_ordered_combined)
```
Seems fairly well within the expected bounds. We could definitely run more iterations if we wanted to have a more strict check, but for now, we are happy.

### Extending to more components

To avoid high chances of components with very similar means, we'll make the prior for `mu` much wider. At the same time we concentrate the prior on `theta` to avoid having components with too low probability (and thus not present in the data)

```{r}
model_final <- cmdstan_model("simple_workflow/mixture_final.stan")
backend_final <- cmdstan_sample_SBC_backend(model_final, chains = 2)
```

```{r}
generator_func_final <- function(N, N_components) {
  # If the priors for all components of an ordered vector are the same
  # then just sorting the result of a generator is enough to create
  # a valid sample from the ordered vector
  mu <- sort(rnorm(N_components, 5, 2.5)) 
  # Converting via as.numeric to keep only one dimension
  theta <- as.numeric(MCMCpack::rdirichlet(1, rep(3, N_components)))
  
  y <- numeric(N)
  for(n in 1:N) {
    comp <- sample.int(N_components, size = 1, prob = theta)
    y[n] <- rpois(1, exp(mu[comp]))
  }
  
  list(
    parameters = list(
      mu = mu,
      theta = theta
    ),
    generated = list(
      N = N,
      N_components = N_components,
      y = y
    )
  )
}

generator_final <- function_SBC_generator(generator_func_final, N = 100, N_components = 3)
```

Quickly test a few datasets to make sure we didn't do any big error (I actually did when writing this :-))

```{r}
set.seed(223345864)
datasets_final_10 <- generate_datasets(generator_final, 10) 
results_final_10 <- compute_results(datasets_final_10, backend_final)
```
```{r}
bayesplot::mcmc_pairs(results_final_10$fits[[1]]$draws(), np = nuts_params(results_final_10$fits[[1]]))
```


This time we don't save the fits to save memory.

```{r}
set.seed(54622625)
datasets_final_200 <- generate_datasets(generator_final, 200) 
results_final_200 <- compute_results(datasets_final, backend_final, keep_fits = FALSE)
```

```{r}
results_final <- bind_results(results_final_10, results_final_200)
results_final_subset <- results_final[results_final$backend_diagnostics$n_divergent == 0]
summary(results_final_subset)
```


## Dirichlet regression component

```{r}
model_dir_first <- cmdstan_model("simple_workflow/dirichlet_first.stan")
backend_dir_first <- cmdstan_sample_SBC_backend(model_dir_first) 

```


```{r}
generator_func_dir_first <- function(N_obs, N_components, N_predictors) {
  beta <- matrix(rnorm(N_predictors * N_components, 0, 2), nrow = N_components, ncol = N_predictors)
  precision <- rlnorm(1, 2, 1)
  
  x <- matrix(rnorm(N_predictors * N_obs, 0, 1), nrow = N_predictors, ncol = N_obs)
  x[1, ] <- 1 # Intercept

  y <- matrix(NA_real, nrow = N_components, ncol = N_obs)
    
  for(n in 1:N_obs) {
    linpred <- rep(0, N_components)
    for(c in 1:N_components) {
      for(p in 1:N_predictors) {
        linpred[c] <- linpred[c] + x[p, n] * beta[c, p]
      }
    }
    mu_vec <- exp(linpred) / sum(exp(linpred))
    y[,n] <- MCMCpack::rdirichlet(1, precision * mu_vec)
  }
    
  list(
    parameters = list(
      beta = beta,
      precision = precision
    ),
    generated = list(
      N_obs = N_obs,
      N_components = N_components,
      N_predictors = N_predictors,
      y = y,
      x = x
    )
  )
}

generator_first <- function_SBC_generator(generator_func_first, N = 50)
```


- Non-identifiability: predictors for all
- Off-by-one: not informed parameter (posterior shrinkage)
- Forgotten prior for intercept?

## Putting it together

TODO: idexing error
