---
title: "Simple model workflow"
output: html_notebook
---

```{r setup}
library(SBC)

# use_cmdstanr <- TRUE # Set to false to use rstan instead
# 
# if(use_cmdstanr) {
#   library(cmdstanr)
# } else {
#   library(rstan)
# }
library(cmdstanr)
library(bayesplot)
library(posterior)

library(future)
plan(multisession) 

options(SBC.min_chunk_size = 5)

```

- Mixture with predictors for ratios

## Mixture component

```{r}
model_first <- cmdstan_model("simple_workflow/mixture_first.stan")
backend_first <- cmdstan_sample_SBC_backend(model_first) 
```

```{r}
generator_func_first <- function(N) {
  mu1 <- rnorm(1, 3, 1)
  mu2 <- rnorm(1, 3, 1)
  theta <- runif(1)
  
  y <- numeric(N)
  for(n in 1:N) {
    if(runif(1) < theta) {
      y[n] <- rpois(1, exp(mu1))
    } else {
      y[n] <- rpois(1, exp(mu2))
    }
  }
  
  list(
    parameters = list(
      mu1 = mu1,
      mu2 = mu2,
      theta = theta
    ),
    generated = list(
      N = N,
      y = y
    )
  )
}

generator_first <- function_SBC_generator(generator_func_first, N = 50)
```

```{r}
set.seed(233598465)
datasets_first <- generate_datasets(generator_first, 1)
```

```{r}
results_first <- compute_results(datasets_first, backend_first)
```

We have divergences, let us examine the pairs plots

```{r}
results_first$stats
mcmc_pairs(results_first$fits[[1]]$draws())
```

One thing that stands out is that either `mu1` is tightly determined and `mu2` is allowed the full prior range or the other way around. We also don't learn anything about theta.

This might be puzzling but relates to bad usage of `log_mix` (TODO explain)

### Fixing mixture

```{r}
model_fixed_log_mix <- cmdstan_model("simple_workflow/mixture_fixed_log_mix.stan")
backend_fixed_log_mix <- cmdstan_sample_SBC_backend(model_fixed_log_mix)
```

```{r}
results_fixed_log_mix <- compute_results(datasets_first, backend_fixed_log_mix)
```

Some slightly problematic Rhats, but nothing obviously wrong, let's run a few more iterations.

```{r}
results_fixed_log_mix$stats
```
```{r}
set.seed(8314566)
datasets_first_10 <- generate_datasets(generator_first, 10)
```

```{r}
results_fixed_log_mix_2 <- compute_results(datasets_first_10, backend_fixed_log_mix)
```

```{r}
hist(results_fixed_log_mix_2$stats$rhat)
```

```{r}
mcmc_pairs(results_fixed_log_mix_2$fits[[1]]$draws())
```

We clearly see two modes. And upon reflection, it is not a lot surprising why: swapping `mu1` with `mu2` while also changing `theta` for `1 - theta` gives _exactly_ the same likelihood - because the ordering does not matter. A more detailed explanation of this type of problem is at https://betanalpha.github.io/assets/case_studies/identifying_mixture_models.html

### Fixing ordering

We can easily fix the ordering of the `mu`s by using the `ordered` built-in type.

```{r}
model_fixed_ordered <- cmdstan_model("simple_workflow/mixture_fixed_ordered.stan")
backend_fixed_ordered <- cmdstan_sample_SBC_backend(model_fixed_ordered) 
```
We also need to update the generator to match the new names and ordering constant:

```{r}
generator_func_ordered <- function(N) {
  # If the priors for all components of an ordered vector are the same
  # then just sorting the result of a generator is enough to create
  # a valid sample from the ordered vector
  mu <- sort(rnorm(2, 3, 1)) 
  theta <- runif(1)
  
  y <- numeric(N)
  for(n in 1:N) {
    if(runif(1) < theta) {
      y[n] <- rpois(1, exp(mu[1]))
    } else {
      y[n] <- rpois(1, exp(mu[2]))
    }
  }
  
  list(
    parameters = list(
      mu = mu,
      theta = theta
    ),
    generated = list(
      N = N,
      y = y
    )
  )
}

generator_ordered <- function_SBC_generator(generator_func_ordered, N = 50)
```

We are kind of confident (and the model fits quickly), so we'll already start with 10 datasets.

```{r}
set.seed(3785432)
datasets_ordered_10 <- generate_datasets(generator_ordered, 10)
```


```{r}
results_fixed_ordered <- compute_results(datasets_ordered_10, backend_fixed_ordered)
```


```{r}
results_fixed_ordered$stats
```
There is one fit with high R-hats and low ESS. Let's look at the pairs plot:

```{r}
problematic_fit <- 2
mcmc_pairs(results_fixed_ordered$fits[[problematic_fit]]$draws())
```

There is a lot of ugly stuff going on. Notably, one can notice that the posterior of theta is bimodal, preferring either almost 0 or almost 1 - and when that happens, the mean of one of the components is almost unconstrained. 
Why does that happen? The key to the answer is in the simulated values for the component means:

```{r}
subset_draws(datasets_ordered_10$parameters, draw = problematic_fit)
```
We were unlucky enough to simulate a dataset where both components have almost the same mean and thus we are actually looking at a dataset that is not really a mixture. Mixture models can misbehave badly in such cases (see once again the https://betanalpha.github.io/assets/case_studies/identifying_mixture_models.html#5_singular_components_and_computational_issues) for a bit more detailed dive into this particular problem.

### Fixing degenerate components?

What to do about this? Fixing the model to handle such cases gracefully is hard. But the problem is basically our prior - we want to express that (since we are fitting a two component model), we don't expect the means to be too similar. So if we can change our simulation to avoid this, we'll be able to proceed with SBC. If such a pattern appeared in real data, we would still have a problem, but we would notice thanks to the diagnostics.

This can definitely be done. But another way is to just ignore the datasets that had divergences for SBC calculations.

So we can use the same construction Stan uses to build the `ordered` parameter (https://mc-stan.org/docs/2_27/reference-manual/ordered-vector.html), that is the first element is unconstrained and then we parametrize in terms of differences, which are bound to be positive.

It might even make sense to relax the prior when actually using the model. It is rare --- although not impossible --- for a minor change in prior to break a model that otherwise behaves well. And it is still better to use a model that was checked thoroughly (albeit with a bit different prior), than a model that was  


## Dirichlet regression component

- Non-identifiability: predictors for all
- Off-by-one: not informed parameter
- Forgotten prior for intercept?

## Putting it together

TODO: idexing error
