---
title: "SBC for approximate methods in Stan  (ADVI, optimizing)"
author: "Hyunji Moon, Martin ModrÃ¡k"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteIndexEntry{SBC for approximate methods in Stan  (ADVI, optimizing)}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---


```{r setup, message=FALSE,warning=FALSE, results="hide"}
library(SBC); 
use_cmdstanr <- TRUE # Set to false to use rstan instead

if(use_cmdstanr) {
  library(cmdstanr)
} else {
  library(rstan)
}

options(mc.cores = parallel::detectCores())

# Parallel processing

library(future)
plan(multisession)

# The fits are very fast,
# so we force a minimum chunk size to reduce overhead of
# paralellization and decrease computation time.
options(SBC.min_chunk_size = 5)


# Setup caching of results
cache_dir <- "./approximate_computation_SBC_cache"
if(!dir.exists(cache_dir)) {
  dir.create(cache_dir)
}

```


# 1. Intro.

HMC can be slow either from large data or difficult geometry. For simple geometries (and large data) the ADVI (automatic differentiation variational inference) approximation can be helpful. SBC provides one standard to test whether ADVI works well for your model without ever needing to run full
HMC for your model. 

# 2. Comparing HMC and ADVI 
Simple model with location and scale parameter for normal distirbution is used and coded into generator and backend (stan file).


```{r}
generator_contamination <- function(N) {
  mu_background <- rlnorm(1, -2, 0.2)
  mu_signal <- rlnorm(1, 2, 1)

  p_signal <- rbeta(1, 2, 2)

  mu <- c(mu_background, mu_background + mu_signal)
  y <- rpois(N, ifelse(runif(N) < p_signal, mu_background + mu_signal, mu_background))
  
  list(
    parameters = list(
      mu_signal = mu_signal,
      mu_background = mu_background,
      p_signal = p_signal
    ),
    generated = list(
      N = N,
      y = y
    )
  )
}


model_contamination <- cmdstan_model("stan/contaminated_poisson.stan")
backend_contamination <- SBC_backend_cmdstan_variational(model_contamination, n_retries_init = 3)

```

```{r}
set.seed(46522641)
ds_contamination <- generate_datasets(SBC_generator_function(generator_contamination, N = 100), n_datasets = 100)
res_contamination <- compute_results(ds_contamination, backend_contamination, cache_mode = "results", cache_location = file.path(cache_dir, "contamination"))
```

```{r}
plot_ecdf_diff(res_contamination)
plot_rank_hist(res_contamination)
plot_sim_estimated(res_contamination)
```

```{r}
set.seed(12365455)
ds_contamination_2 <- generate_datasets(SBC_generator_function(generator_contamination, N = 100), n_datasets = 900)
res_contamination_2 <- bind_results(
  res_contamination,
  compute_results(ds_contamination_2, backend_contamination, cache_mode = "results", cache_location = file.path(cache_dir, "contamination_2"))
)
  
```


```{r}
plot_ecdf_diff(res_contamination_2)
plot_rank_hist(res_contamination_2)
plot_sim_estimated(res_contamination_2, alpha = 0.2)
```


```{r}
plot_coverage(res_contamination_2)
```

```{r}
res_contamination_fullrank <- compute_results(bind_datasets(ds_contamination, ds_contamination_2), SBC_backend_cmdstan_variational(model_contamination, n_retries_init = 3, algorithm = "fullrank"),
                                              cache_mode = "results",
                                              cache_location = file.path(cache_dir, "contamination_fullrank"))
```

```{r}
plot_ecdf_diff(res_contamination_fullrank)
plot_rank_hist(res_contamination_fullrank)
plot_sim_estimated(res_contamination_fullrank)
```

```{r}
plot_coverage(res_contamination_fullrank)
```


```{r}
res_contamination_mcmc <- compute_results(bind_datasets(ds_contamination, ds_contamination_2), SBC_backend_cmdstan_sample(model_contamination, chains = 2),
                                          keep_fits = FALSE,
                                          cache_mode = "results",
                                          cache_location = file.path(cache_dir, "contamination_mcmc"))
```

```{r}
plot_ecdf_diff(res_contamination_mcmc)
plot_rank_hist(res_contamination_mcmc)
plot_sim_estimated(res_contamination_mcmc)
```


https://mc-stan.org/users/documentation/case-studies/hmm-example.html

Simplified to two states and slightly tighter prior on transition probs

```{r}
generator_HMM <- function(N) {
  # You can simulate i.i.d. ordered vector just by sorting
  mu_background <- rlnorm(1, -2, 0.2)
  mu_signal <- rlnorm(1, 2, 1)

  # Draw the transition probabilities
  t1 <- MCMCpack::rdirichlet(1, c(3, 3))
  t2 <- MCMCpack::rdirichlet(1, c(3, 3))

  states = rep(NA_integer_, N)
  # Draw from initial state distribution
  rho <- MCMCpack::rdirichlet(1, c(1, 10))

  states[1] = sample(1:2, size = 1, prob = rho)
  for(n in 2:length(states)) {
    if(states[n - 1] == 1)
      states[n] = sample(c(1, 2), size = 1, prob = t1)
    else if(states[n - 1] == 2)
      states[n] = sample(c(1, 2), size = 1, prob = t2)
  }  
  
  mu <- c(mu_background, mu_background + mu_signal)
  y <- rpois(N, mu[states])
  
  list(
    parameters = list(
      mu_signal = mu_signal,
      mu_background = mu_background,
      # rdirichlet returns matrices, convert to 1D vectors
      t1 = as.numeric(t1),
      t2 = as.numeric(t2),
      rho = as.numeric(rho)
    ),
    generated = list(
      N = N,
      y = y
    )
  )
}


model_HMM <- cmdstan_model("stan/hmm_contaminated_poisson.stan")
backend_HMM <- SBC_backend_cmdstan_variational(model_HMM, n_retries_init = 3)

```

```{r}
set.seed(2165498)
ds_hmm <- generate_datasets(SBC_generator_function(generator_HMM, N = 100), n_datasets = 20)
res_hmm <- compute_results(ds_hmm, backend_HMM,
                           cache_mode = "results", cache_location = file.path(cache_dir, "hmm"))
```

```{r}
plot_ecdf_diff(res_hmm)
plot_rank_hist(res_hmm)
plot_sim_estimated(res_hmm)
```
```{r}
res_mcmc <- compute_results(ds, SBC_backend_cmdstan_sample(model_HMM, chains = 2))
```

```{r}
plot_ecdf_diff(res_mcmc)
plot_rank_hist(res_mcmc)
plot_sim_estimated(res_mcmc)
```


```{R generator and backend}
generator <- function(hyperparam, param, predictor = NULL){
  # hyperparamter
  N = hyperparam$N
  prior_width = hyperparam$prior_width
  # paramter
  loc = param$loc
  scale = param$scale
  # predictor
  # generate
  S = ndraws(param[[1]])
  y <- rfun(rnorm) (n = N, mean = loc, sd = scale) 
  gen_rvars <- draws_rvars(N = N, y = y, prior_width = prior_width)
  # SBC-type reshape e.g. loc rvar<S>[1] distributed to each parallel simulation as rvar<1>[1]
  SBC_datasets(
    parameters = as_draws_matrix(param), 
    generated = draws_rvars_to_standata(gen_rvars)
  )
}

tn <- "
data {
  int N;
  vector[N] y;
  real<lower=0> prior_width;
}

parameters {
  real loc;
  real <lower = 0> scale;
}

model {
  loc ~ normal(0, prior_width);
  scale ~ lognormal(0, prior_width);
  y ~ normal(loc, scale);
}
"
mod_two_norm = cmdstanr::cmdstan_model(stan_file = write_stan_file(tn))
```

Comparing SBC with two different backends (HMC and ADVI), the latter is around five times faster (3 and 0.6 min.) for (S, M) = (1000, 100) where `S` is the number of initial parameter values (datasets) and `M` is posterior samples. Current Stan ADVI implementation samples from iid normal which explains the difference in `s_thin_ranks` and `v_thin_ranks` as 10 and 1. If certian correlation is detected, thinning is possible with `recompute_statistics` function without refitting. See appendix for instructions on VI variants aiming for first or higher order corrections.

The following messages is ADVI-specific and explains how ELBO value and its gradient evolved with the heuristically set stepsize `eta`. For further ADVI diagnostics, see [this](https://mc-stan.org/docs/2_27/reference-manual/stochastic-gradient-ascent.html) Stan manual. 
```
Iteration:   1 / 250 [  0%]  (Adaptation) 
Iteration:  50 / 250 [ 20%]  (Adaptation) 
Iteration: 100 / 250 [ 40%]  (Adaptation) 
Success! Found best value [eta = 100] earlier than expected. 
Begin stochastic gradient ascent. 
  iter             ELBO   delta_ELBO_mean   delta_ELBO_med   notes  
   100          -57.517             1.000            1.000 
   200         -528.092             0.946            1.000 
   300         -239.071             1.033            1.000 
```

```{R}
library(posterior)
S = 30
N = 20
M = 100
s_thin_ranks = 10
v_thin_ranks = 1
chains = 4
backend_tn <- SBC_backend_cmdstan_sample(mod_two_norm, chains = chains, iter_sampling = M * s_thin_ranks / chains)
backend_tn_vb <- SBC_backend_cmdstan_variational(mod_two_norm, grad_samples = 200, init = 0.5, n_retries_init = 3)
prior_width = 1
datasets_tn <- generator(
  hyperparam = list(prior_width = prior_width, N = N),
  param = draws_rvars(loc = rvar(rnorm(S, 0, prior_width)), scale = rvar(exp(rnorm(S, 0, prior_width))))
)
#result_tn <- compute_results(datasets_tn, backend_tn)
result_tn_vb <- compute_results(datasets_tn, backend_tn_vb, thin_ranks = v_thin_ranks)
```

Rank results for HMC is acceptable while for ADVI indicates under-dispersion. Variational inference that measures the divergence based on the true distribution (e.g. KL(approx.||true)) is known for its compactness (Turner, 2010) and therefore have tendency to be under-dispersed. However, Yao et al. (2018) notes VI posteriors can be both over-dispersed and under-dispersed, on which parameter region SBC is exploring. Rank plot indicates that ADVI for this problem setting could be inappropriate i.e. the three components, given parameter values, outcome model, and inference engine are not self-consistent. Adjusting the computation algorithm via hyperparameters that determine convergence standards can be one solution. 

```{r}
plot_rank_hist(result_tn) + ggtitle("HMC")
plot_ecdf_diff(result_tn) + ggtitle("HMC")
plot_rank_hist(result_tn_vb) + ggtitle("Variational - meanfield")
plot_ecdf_diff(result_tn_vb) + ggtitle("Variational - meanfield")
```

# 3. Adjusting algorithm hyperparameters
One ADVI hyperparater is `tol_rel_obj` which is a convergence tolerance on the relative norm of the objective. Stan's default is 0.001 so decreasing it to 0.0001 takes longer (fifth power didn't end) but provides better rank plots.

```{R}
backend_tn_vb_dot1pw4tol <- SBC_backend_cmdstan_variational(mod_two_norm, tol_rel_obj = 0.0001, grad_samples = 200, init = 0.5)
result_tn_vb_dot1pw4tol <- compute_results(datasets_tn, backend_tn_vb_dot1pw4tol)
plot_rank_hist(result_tn_vb_dot1pw4tol)
plot_ecdf_diff(result_tn_vb_dot1pw4tol)
```

# Notes on VI variants
If importance resampling is used to reduce the bias of ADVI, there is dependency, as some original draws can be drawn many times. Instead of thinning, a specific type of stratified importance resampling could be used to produce a sample that has ESS close to the nominal size. Base on experiments if ESS is within +-10% of the nominal size, bias in extreme ranks is already quite small which is why ESS for several quantiles needs checking. For instance, ESS for mean can be much higher than ESS for tails and the final ESS is recommended to target for the worst performing statistics (i.e. ranks computable variables).

# Reference
- Turner and Sahani (2010) http://www.gatsby.ucl.ac.uk/~maneesh/papers/turner-sahani-2010-ildn.pdf
- Yao et al. (2018) http://proceedings.mlr.press/v80/yao18a/yao18a.pdf
