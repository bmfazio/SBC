---
title: Rejection sampling in dataset generation
author: "Martin ModrÃ¡k"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteIndexEntry{Rejection sampling in dataset generation}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

How does rejection sampling when generating datasets affect the validity of SBC?

TODO: copy math and ideas from
https://discourse.mc-stan.org/t/using-narrower-priors-for-sbc/21709/6?u=martinmodrak


```{r setup, message=FALSE,warning=FALSE, results="hide"}
library(SBC)

# use_cmdstanr <- TRUE # Set to false to use rstan instead
# 
# if(use_cmdstanr) {
#   library(cmdstanr)
# } else {
#   library(rstan)
# }
library(cmdstanr)
library(bayesplot)
library(posterior)

library(future)
plan(multisession) 

options(SBC.min_chunk_size = 10)

# Setup caching of results
cache_dir <- "./rejection_sampling_SBC_cache"
if(!dir.exists(cache_dir)) {
  dir.create(cache_dir)
}

```


We'll use a very simple model throughout this vignette:

```{r}
stan_code <- "
data {
   int<lower=0> N;
   real y[N];
}

parameters {
   real mu;
}

model {
   mu ~ normal(0, 2);
   y ~ normal(mu, 1);
}

"

backend <- SBC_backend_cmdstan_sample(cmdstan_model(write_stan_file(stan_code)), iter_warmup = 800, iter_sampling = 800)
```

# No rejections

First, we'll use a generator that matches the model exactly.

```{r}
N <- 10
generator <- SBC_generator_function(function() {
   mu <- rnorm(1, 0, 2)
   list(
     parameters = list(mu = mu),
     generated = list(N = N, y = rnorm(N, mu, 1))
   )
})
```

So we expect the SBC to pass even with a large number of fits.

```{r}
set.seed(2323455)
datasets <- generate_datasets(generator, 1000)
```

```{r}
results <- compute_results(datasets, backend, keep_fits = FALSE, 
                    cache_mode = "results", 
                    cache_location = file.path(cache_dir, "no_rejections"))
```

```{r}
plot_ecdf_diff(results)
plot_rank_hist(results)
```

Indeed, all looks good.

# Rejection based on parameter values

Now let us modify the generator to reject based on parameter values. 

```{r}
generator_reject_param <- SBC_generator_function(function() {
   repeat {
    mu <- rnorm(1, 0, 2)
    if(mu > 3) {
      break
    }
   }
   list(
     parameters = list(mu = mu),
     generated = list(N = N, y = rnorm(N, mu, 1))
   )
})
```

We don't even need to run very many fits to see the problem.

```{r}
set.seed(21455)
datasets_reject_param <- generate_datasets(generator_reject_param, 200)
```

```{r}
results_reject_param <- compute_results(datasets_reject_param, backend, keep_fits = FALSE, 
                    cache_mode = "results", 
                    cache_location = file.path(cache_dir, "reject_param"))
```

```{r}
plot_ecdf_diff(results_reject_param)
plot_rank_hist(results_reject_param)
```

Indeed, we see a clear failure.

# Rejecting based on data

But what if we reject based on the values of data? This should in theory result in just
a constant change in posterior density and not affect SBC. (SBC will however then check only the 
non-rejected parts of the data space). We will do a relatively aggressive rejection scheme (reject more than 50% of datasets).

```{r}
generator_reject_y <- SBC_generator_function(function() {
   repeat {
    mu <- rnorm(1, 0, 2)
    y <- rnorm(N, mu, 1)
    if(mean(y) > 5) {
      break
    }
   }
   list(
     parameters = list(mu = mu),
     generated = list(N = N, y = y)
   )
})
```

```{r}
set.seed(369654)
datasets_reject_y <- generate_datasets(generator_reject_y, 1000)
```

```{r}
results_reject_y <- compute_results(datasets_reject_y, backend, keep_fits = FALSE, 
                    cache_mode = "results", 
                    cache_location = file.path(cache_dir, "reject_y"))
```

```{r}
plot_ecdf_diff(results_reject_y)
```

We see that even with quite heavy rejection based on y, SBC to a high resolution passes.

# Take home message

If our priors can sometimes generate datasets that are unrealistic, but we are unable to 
specify a better prior directly (e.g. because we would need to define some sort of joint prior),
we can use rejection sampling to prune unrealistic datasets as long as we only filter by the observed
data and don't directly use any unobserved parameter values. Notably, filtering based on divergences or other
fitting issues is also just a function of data and thus permissible. The resulting SBC will however provide guarantees
only for datasets that would not be rejected by the same criteria.
